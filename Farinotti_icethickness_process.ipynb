{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "import os\n",
    "import argparse\n",
    "# External libraries\n",
    "from osgeo import gdal\n",
    "#import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import shapely\n",
    "\n",
    "from pygeotools.lib import iolib, warplib, geolib, timelib, malib\n",
    "\n",
    "import pygemfxns_modelsetup as modelsetup\n",
    "\n",
    "\n",
    "#Function to generate a 3-panel plot for input arrays\n",
    "def plot3panel(dem_list, clim=None, titles=None, cmap='inferno', label=None, overlay=None, fn=None):\n",
    "    fig, axa = plt.subplots(1,3, sharex=True, sharey=True, figsize=(10,5))\n",
    "    alpha = 1.0\n",
    "    for n, ax in enumerate(axa):\n",
    "        #Gray background\n",
    "        ax.set_facecolor('0.5')\n",
    "        #Force aspect ratio to match images\n",
    "        ax.set(aspect='equal')\n",
    "        #Turn off axes labels/ticks\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if titles is not None:\n",
    "            ax.set_title(titles[n])\n",
    "        #Plot background shaded relief map\n",
    "        if overlay is not None:\n",
    "            alpha = 0.7\n",
    "            axa[n].imshow(overlay[n], cmap='gray', clim=(1,255))\n",
    "    #Plot each array\n",
    "    im_list = [axa[i].imshow(dem_list[i], clim=clim, cmap=cmap, alpha=alpha) for i in range(len(dem_list))]\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(im_list[0], ax=axa.ravel().tolist(), label=label, extend='both', shrink=0.5)\n",
    "    if fn is not None:\n",
    "        fig.savefig(fn, bbox_inches='tight', pad_inches=0, dpi=150)\n",
    "\n",
    "#Input DEM filenames\n",
    "dem_ref_fn = None\n",
    "# dem_ref_fn = '/Users/davidrounce/Documents/Dave_Rounce/HiMAT/DEMs/Alaska_albers_V3_mac/Alaska_albers_V3.tif'\n",
    "thickness_fp_prefix = ('/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/' +\n",
    "                       'composite_thickness_RGI60-all_regions/')\n",
    "# dem_farinotti_fp = ('/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/surface_DEMs_RGI60/' +\n",
    "#                     'surface_DEMs_RGI60-01/')\n",
    "dem_farinotti_fp_prefix = '/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/surface_DEMs_RGI60/'\n",
    "output_fp = '/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/output/'\n",
    "fig_fp = output_fp + 'figures/'\n",
    "if os.path.exists(output_fp) == False:\n",
    "    os.makedirs(output_fp)\n",
    "if os.path.exists(fig_fp) == False:\n",
    "    os.makedirs(fig_fp)\n",
    "\n",
    "rgi_regionsO1 = [15]                 # RGI Order 1 regions\n",
    "binsize = 10                        # elevation bin (must be an integer greater than 1)\n",
    "dem_poorquality_switch = True       # Switch to filter poor quality DEMs if another DEM is available\n",
    "dem_poorquality_threshold = 200     # threshold used to identify problems with Farinotti DEM\n",
    "option_plot_DEMsraw = True          # Option to plot the raw DEMs\n",
    "option_plot_DEMs = False             # Option to plot the masked DEMs\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DELETE ME - SWITCH TO COMPLETE LIST\n",
      "\n",
      "\n",
      "1 glaciers in region 15 are included in this model run: ['02228']\n",
      "This study is focusing on 1 glaciers in region [15]\n",
      "['/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/surface_DEMs_RGI60/surface_DEMs_RGI60-15/surface_DEM_RGI60-15.02228.tif', '/Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/composite_thickness_RGI60-all_regions/RGI60-15/RGI60-15.02228_thickness.tif']\n",
      "\n",
      "Warping all inputs to the following:\n",
      "Resolution: 25.0\n",
      "Extent: [751262.5, 3110587.5, 755412.5, 3112962.5]\n",
      "Projection: '+proj=utm +zone=45 +datum=WGS84 +units=m +no_defs '\n",
      "Resampling alg: cubic\n",
      "\n",
      "1 of 2: /Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/surface_DEMs_RGI60/surface_DEMs_RGI60-15/surface_DEM_RGI60-15.02228.tif\n",
      "2 of 2: /Users/davidrounce/Documents/Dave_Rounce/HiMAT/IceThickness_Farinotti/composite_thickness_RGI60-all_regions/RGI60-15/RGI60-15.02228_thickness.tif\n",
      "0 15.02228 520 1150\n"
     ]
    }
   ],
   "source": [
    "# ======\n",
    "glacno_wpoor_DEM = []\n",
    "for region in rgi_regionsO1:\n",
    "\n",
    "    thickness_fp = thickness_fp_prefix + 'RGI60-' + str(region).zfill(2) + '/'\n",
    "    dem_farinotti_fp = dem_farinotti_fp_prefix + 'surface_DEMs_RGI60-' + str(region).zfill(2) + '/'\n",
    "\n",
    "    glacno_list = []\n",
    "    for i in os.listdir(thickness_fp):\n",
    "        if i.endswith('_thickness.tif'):\n",
    "            glacno_list.append(i.split('-')[1].split('_')[0])\n",
    "    glacno_list = sorted(glacno_list)\n",
    "\n",
    "    print('\\n\\nDELETE ME - SWITCH TO COMPLETE LIST\\n\\n')\n",
    "    glacno_list = ['15.02228']\n",
    "    # glacno_list = glacno_list[10000:10010]\n",
    "\n",
    "    # Load RGI glacier data\n",
    "    main_glac_rgi = modelsetup.selectglaciersrgitable(glac_no=glacno_list)\n",
    "    # setup empty datasets\n",
    "    elev_bins_all = np.arange(binsize / 2, main_glac_rgi.Zmax.max() + binsize / 2, binsize).astype(int)\n",
    "    df_cns = ['RGIId']\n",
    "    for elev_bin in elev_bins_all:\n",
    "        df_cns.append(elev_bin)\n",
    "    main_glac_hyps = pd.DataFrame(np.zeros((main_glac_rgi.shape[0], len(df_cns))), columns=df_cns)\n",
    "    main_glac_thickness = pd.DataFrame(np.zeros((main_glac_rgi.shape[0], len(df_cns))), columns=df_cns)\n",
    "    main_glac_width = pd.DataFrame(np.zeros((main_glac_rgi.shape[0], len(df_cns))), columns=df_cns)\n",
    "    main_glac_length = pd.DataFrame(np.zeros((main_glac_rgi.shape[0], len(df_cns))), columns=df_cns)\n",
    "    main_glac_slope = pd.DataFrame(np.zeros((main_glac_rgi.shape[0], len(df_cns))), columns=df_cns)\n",
    "    main_glac_hyps['RGIId'] = main_glac_rgi.RGIId.values\n",
    "    main_glac_thickness['RGIId'] = main_glac_rgi.RGIId.values\n",
    "    main_glac_width['RGIId'] = main_glac_rgi.RGIId.values\n",
    "    main_glac_length['RGIId'] = main_glac_rgi.RGIId.values\n",
    "    main_glac_slope['RGIId'] = main_glac_rgi.RGIId.values\n",
    "        \n",
    "    # ===== PROCESS EACH GLACIER ======\n",
    "    for nglac, glacno in enumerate(glacno_list):\n",
    "        # print(nglac, glacno)\n",
    "        thickness_fn = thickness_fp + 'RGI60-' + glacno + '_thickness.tif'\n",
    "        dem_farinotti_fn = dem_farinotti_fp + 'surface_DEM_RGI60-' + glacno + '.tif'\n",
    "        \n",
    "        # Reproject, resample, warp rasters to common extent, grid size, etc.\n",
    "        #  note: use thickness for the reference to avoid unrealistic extrapolations, e.g., negative thicknesses\n",
    "        #        also using equal area increases areas significantly compared to RGI\n",
    "        raster_fn_list = [dem_farinotti_fn, thickness_fn]\n",
    "        if dem_ref_fn is not None:\n",
    "            raster_fn_list.append(dem_ref_fn)\n",
    "        \n",
    "        print(raster_fn_list)\n",
    "        \n",
    "        ds_list = warplib.memwarp_multi_fn(raster_fn_list, extent='intersection', res='min', t_srs=thickness_fn)\n",
    "\n",
    "        # masked arrays using ice thickness estimates\n",
    "        if dem_ref_fn is not None:\n",
    "            dem_ref_raw, dem_far_raw, thickness = [iolib.ds_getma(i) for i in ds_list]\n",
    "            dem_ref = dem_ref_raw.copy()\n",
    "            dem_ref.mask = thickness.mask\n",
    "        else:\n",
    "            dem_far_raw, thickness = [iolib.ds_getma(i) for i in ds_list]\n",
    "        dem_far = dem_far_raw.copy()\n",
    "        dem_far.mask = thickness.mask\n",
    "        \n",
    "        # DEM selection for binning computations\n",
    "        # if exceeds threshold, then use the reference\n",
    "        if ((abs(main_glac_rgi.loc[nglac,'Zmin'] - dem_far.min()) > dem_poorquality_threshold or\n",
    "             abs(main_glac_rgi.loc[nglac,'Zmax'] - dem_far.max()) > dem_poorquality_threshold) \n",
    "             and dem_ref_fn is not None):\n",
    "            print('  Check Glacier ' + glacno + ': use Christian DEM instead of Farinotti')\n",
    "            print('\\n     RGI Zmin/Zmax:', main_glac_rgi.loc[nglac,'Zmin'], '/', main_glac_rgi.loc[nglac,'Zmax'])\n",
    "            print('     Farinotti Zmin/Zmax:', np.round(dem_far.min(),0), '/', np.round(dem_far.max(),0))\n",
    "            print('     Christian Zmin/Zmax:', np.round(dem_ref.min(),0), '/', np.round(dem_ref.max(),0), '\\n')\n",
    "            glacno_wpoor_DEM.append(glacno)\n",
    "            dem = dem_ref\n",
    "            dem_raw = dem_ref_raw\n",
    "\n",
    "            # ===== PLOT DEMS TO CHECK =====\n",
    "            if option_plot_DEMsraw:\n",
    "                dem_list_raw = [dem_ref_raw, dem_far_raw, thickness]\n",
    "                titles = ['DEM-Christian-raw', 'DEM-Farinotti-raw', 'Thickness']\n",
    "                clim = malib.calcperc(dem_list_raw[0], (2,98))\n",
    "                plot3panel(dem_list_raw, clim, titles, 'inferno', 'Elevation (m WGS84)', fn=fig_fp + glacno +\n",
    "                           '_dem_raw.png')\n",
    "\n",
    "            if option_plot_DEMs:\n",
    "                dem_list = [dem_ref, dem_far, thickness]\n",
    "                titles = ['DEM-Christian', 'DEM-Farinotti', 'Thickness']\n",
    "                clim = malib.calcperc(dem_list[0], (2,98))\n",
    "                plot3panel(dem_list, clim, titles, 'inferno', 'Elevation (m WGS84)', fn=fig_fp + glacno + '_dem.png')\n",
    "        # otherwise, use Farinotti\n",
    "        else:\n",
    "            dem = dem_far\n",
    "            dem_raw = dem_far_raw\n",
    "            \n",
    "        #Extract x and y pixel resolution (m) from geotransform\n",
    "        gt = ds_list[0].GetGeoTransform()\n",
    "        px_res = (gt[1], -gt[5])\n",
    "        #Calculate pixel area in m^2\n",
    "        px_area = px_res[0]*px_res[1]\n",
    "\n",
    "        if debug:\n",
    "            print('\\nx_res [m]:', np.round(px_res[0],1), 'y_res[m]:', np.round(px_res[1],1),'\\n')\n",
    "\n",
    "        # ===== USE SHAPEFILE OR SINGLE POLYGON TO CLIP =====\n",
    "        # shp_fn = '/Users/davidrounce/Documents/Dave_Rounce/HiMAT/RGI/rgi60/01_rgi60_Alaska/01_rgi60_Alaska.shp'\n",
    "        # #Create binary mask from polygon shapefile to match our warped raster datasets\n",
    "        # shp_mask = geolib.shp2array(shp_fn, ds_list[0])\n",
    "        # #Now apply the mask to each array\n",
    "        # dem_list_shpclip = [np.ma.array(dem, mask=shp_mask) for dem in dem_list]\n",
    "        # plot3panel(dem_list_shpclip, clim, titles, 'inferno', 'Elevation (m WGS84)', fn=output_fp + 'dem_shpclp.png')\n",
    "        # rgi_alaska = gpd.read_file(shp_fn)\n",
    "        # print(rgi_alaska.head())\n",
    "        # rgi_alaska.plot();\n",
    "        # print(rgi_alaska.crs)\n",
    "        # # print('\\nGeometry_type:\\n',rgi_alaska[0:5].geom_type)\n",
    "        # # print('\\nArea (NOTE THESE ARE IN DEGREES!):\\n',rgi_alaska[0:5].geometry.area)\n",
    "        # # print('\\nBounds:\\n',rgi_alaska[0:5].geometry.bounds)\n",
    "        # rgi_alaska.plot(column='O2Region', categorical=True, legend=True, figsize=(14,6))\n",
    "        # rgiid = 'RGI60-' + glacno\n",
    "        # rgi_single = rgi_alaska[rgi_alaska['RGIId'] == rgiid]\n",
    "        # # export to\n",
    "        # rgi_single_fn = 'rgi_single.shp'\n",
    "        # rgi_single.to_file(rgi_single_fn)\n",
    "        # #Create binary mask from polygon shapefile to match our warped raster datasets\n",
    "        # rgi_single_mask = geolib.shp2array(rgi_single_fn, ds_list[0])\n",
    "        # #Now apply the mask to each array\n",
    "        # dem_list_shpclip = [np.ma.array(dem, mask=rgi_single_mask) for dem in dem_list]\n",
    "        # plot3panel(dem_list_shpclip, clim, titles, 'inferno', 'Elevation (m WGS84)', fn=output_fp + 'dem_single.png')\n",
    "        # =============================================================================================================\n",
    "\n",
    "        if debug:\n",
    "            glacier_area_total = thickness.count() * px_res[0] * px_res[1] / 10**6\n",
    "            print(glacno, 'glacier area [km2]:', np.round(glacier_area_total,2),\n",
    "                  'vs RGI [km2]:', np.round(main_glac_rgi.loc[nglac,'Area'],2))\n",
    "\n",
    "        # Remove negative elevation values\n",
    "        dem[dem < 0] = 0\n",
    "        dem.mask = thickness.mask\n",
    "\n",
    "        elev_bin_min = binsize * (dem.min() / binsize).astype(int)\n",
    "        elev_bin_max = binsize * (dem.max() / binsize).astype(int) + binsize\n",
    "\n",
    "        print(nglac, glacno, elev_bin_min, elev_bin_max)\n",
    "\n",
    "        # if elev_bin_min < 0:\n",
    "        #     print(nglac, glacno, elev_bin_min, elev_bin_max)\n",
    "        #     debug_fp = input.output_sim_fp + 'debug/'\n",
    "        #     # Create filepath if it does not exist\n",
    "        #     if os.path.exists(debug_fp) == False:\n",
    "        #         os.makedirs(debug_fp)\n",
    "        #     debug_df = pd.DataFrame(np.zeros((1,1)), columns=['count'])\n",
    "        #     debug_df.iloc[0,0] = 1\n",
    "        #     debug_fn_loaded = str(glacno) + '_nglac' + str(nglac) + '_minlt0_.csv'\n",
    "        #     debug_df.to_csv(debug_fp + debug_fn_loaded)\n",
    "\n",
    "        elev_bin_edges = np.arange(elev_bin_min, elev_bin_max+binsize, binsize)\n",
    "        elev_bins = (elev_bin_edges[0:-1] + binsize/2).astype(int)\n",
    "        \n",
    "        # Hypsometry [km2]\n",
    "        #  must used .compressed() in histogram to exclude masked values\n",
    "        hist, elev_bin_edges = np.histogram(dem.reshape(-1).compressed(), bins=elev_bin_edges)\n",
    "        bin_hyps = hist * px_res[0] * px_res[1] / 10**6\n",
    "        if debug:\n",
    "            print('Zmin/Zmax:', np.round(dem.min(),0), '/', np.round(dem.max(),0), '\\n')\n",
    "            print('elev_bin_edges:', elev_bin_edges)\n",
    "            print('hist:', hist)\n",
    "            print('total area:', hist.sum() * px_res[0] * px_res[1] / 10**6)\n",
    "\n",
    "        # Mean thickness [m]\n",
    "        hist_thickness, elev_bin_edges = np.histogram(dem.reshape(-1).compressed(), bins=elev_bin_edges,\n",
    "                                                      weights=thickness.reshape(-1).compressed())\n",
    "        bin_thickness = hist_thickness / hist\n",
    "\n",
    "        # Mean Slope [deg]\n",
    "        # --> MAY WANT TO RESAMPLE TO SMOOTH DEM PRIOR TO ESTIMATING SLOPE\n",
    "        grad_x, grad_y = np.gradient(dem_raw, px_res[0], px_res[1])\n",
    "        slope = np.arctan(np.sqrt(grad_x ** 2 + grad_y ** 2))\n",
    "        slope_deg = np.rad2deg(slope)\n",
    "        slope_deg.mask = dem.mask\n",
    "        hist_slope, elev_bin_edges = np.histogram(dem.reshape(-1).compressed(), bins=elev_bin_edges,\n",
    "                                                  weights=slope_deg.reshape(-1).compressed())\n",
    "        bin_slope = hist_slope / hist\n",
    "\n",
    "        # Length [km] - based on the mean slope and bin elevation\n",
    "        bin_length = binsize / np.tan(np.deg2rad(bin_slope)) / 1000\n",
    "\n",
    "        # Width [km] - based on length (inherently slope) and bin area\n",
    "        bin_width = bin_hyps / bin_length\n",
    "        \n",
    "        # Remove negative values\n",
    "        bin_hyps[bin_hyps < 0] = 0\n",
    "        bin_thickness[bin_thickness < 0] = 0\n",
    "        bin_width[bin_width < 0] = 0\n",
    "        bin_length[bin_length < 0] = 0\n",
    "        bin_slope[bin_slope < 0] = 0\n",
    "\n",
    "        # Record properties\n",
    "        # Check if need to expand columns\n",
    "        missing_cns = sorted(list(set(elev_bins) - set(df_cns)))\n",
    "        if len(missing_cns) > 0:\n",
    "            for missing_cn in missing_cns:\n",
    "                main_glac_hyps[missing_cn] = 0\n",
    "                main_glac_thickness[missing_cn] = 0\n",
    "                main_glac_width[missing_cn] = 0\n",
    "                main_glac_length[missing_cn] = 0\n",
    "                main_glac_slope[missing_cn] = 0\n",
    "        # Record data\n",
    "        main_glac_hyps.loc[nglac, elev_bins] = bin_hyps\n",
    "        main_glac_thickness.loc[nglac, elev_bins] = bin_thickness\n",
    "        main_glac_width.loc[nglac, elev_bins] = bin_width\n",
    "        main_glac_length.loc[nglac, elev_bins] = bin_length\n",
    "        main_glac_slope.loc[nglac, elev_bins] = bin_slope\n",
    "\n",
    "    # Remove NaN values\n",
    "    main_glac_hyps = main_glac_hyps.fillna(0)\n",
    "    main_glac_thickness = main_glac_thickness.fillna(0)\n",
    "    main_glac_width = main_glac_width.fillna(0)\n",
    "    main_glac_length = main_glac_length.fillna(0)\n",
    "    main_glac_slope = main_glac_slope.fillna(0)\n",
    "#     # Remove negative values\n",
    "#     main_glac_hyps[main_glac_hyps < 0] = 0\n",
    "#     main_glac_thickness[main_glac_thickness < 0] = 0\n",
    "#     main_glac_width[main_glac_width < 0] = 0\n",
    "#     main_glac_length[main_glac_length < 0] = 0\n",
    "#     main_glac_slope[main_glac_slope < 0] = 0\n",
    "    # Export results\n",
    "    main_glac_hyps.to_csv(output_fp + 'area_km2_' + \"{:02d}\".format(region) + '_Farinotti2019_' +\n",
    "                          str(binsize) + 'm.csv', index=False)\n",
    "    main_glac_thickness.to_csv(output_fp + 'thickness_m_' + \"{:02d}\".format(region) + '_Farinotti2019_' +\n",
    "                               str(binsize) + 'm.csv', index=False)\n",
    "    main_glac_width.to_csv(output_fp + 'width_km_' + \"{:02d}\".format(region) + '_Farinotti2019_' +\n",
    "                           str(binsize) + 'm.csv', index=False)\n",
    "    main_glac_length.to_csv(output_fp + 'length_km_' + \"{:02d}\".format(region) + '_Farinotti2019_' +\n",
    "                            str(binsize) + 'm.csv', index=False)\n",
    "    main_glac_slope.to_csv(output_fp + 'slope_deg_' + \"{:02d}\".format(region) + '_Farinotti2019_' +\n",
    "                           str(binsize) + 'm.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
